{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this notebook takes the seperate data from each impact and combines them into one file.\n",
    "#INITIALISATION, FINDING DATA\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "path_113 = \"D:/MASTERS/HELMET_SIMULATIONS\"\n",
    "dir_list = os.listdir(path_113)\n",
    "n = len(dir_list) #number of impacts\n",
    "print(path_113) #path of main folder with 113 impacts inside\n",
    "print(dir_list) #list of impacts\n",
    "print('number of impacts found:', len(dir_list)) #number of impacts\n",
    "\n",
    "#note the data is manipulated as a dictionary and then converted to a dataframe before exporting to csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STRAIN OUTPUTS/ SIMULATION RESULTS\n",
    "\n",
    "#create empty dictionary:\n",
    "data_dict = {}\n",
    "#creating the labels for the data\n",
    "#also want impact type (e.g. nYR),and kinematics metrics (e.g max rot acc).\n",
    "data_dict['Impact label'] = []\n",
    "data_dict['Strain 90th%'] = []\n",
    "data_dict['Strain rate 90th%'] = []\n",
    "\n",
    "print(data_dict) #check it looks right\n",
    "\n",
    "for i in range(n): \n",
    "    #print('i = ', i)\n",
    "    #filling in the dictionary by accessing the individual files for each impact\n",
    "    path_strains = path_113 + '/' + dir_list[i] + '/90thpercentile_results.csv' #main path of files + impact path + csv filename\n",
    "    strain_result_df = pd.read_csv(path_strains,header=None) #data frame extracted from csv (has no column headers)\n",
    "\n",
    "    #individual accessed values assigned to variables\n",
    "    label = strain_result_df.iat[0,0] #label e.g date_helmet_impact_processed\n",
    "    strain = strain_result_df.iat[0,2] #e.g 0.234\n",
    "    strainrate = strain_result_df.iat[1,2] #e.g 0.344\n",
    "\n",
    "    #adding values to the dictionary\n",
    "    data_dict['Impact label'].append(label)\n",
    "    data_dict['Strain 90th%'].append(strain)\n",
    "    data_dict['Strain rate 90th%' ].append(strainrate)\n",
    "\n",
    "    print('data for', label, 'added') #check dictionary looks right\n",
    "\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check: checking matches for different impacts\n",
    "impact_no  = random.randint(0, 113)\n",
    "print(data_dict['Impact label'][impact_no])\n",
    "print(data_dict['Strain 90th%'][impact_no])\n",
    "print(data_dict['Strain rate 90th%'][impact_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a CSV with dictionary so far\n",
    "df = pd.DataFrame(data_dict)\n",
    "df.to_csv('E:/MASTERS/IMPACT_STRAINS.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding INPUTS/FEATURES:\n",
    "\n",
    "#MAX LINEAR and MAX ROTATIONAL acceleration, XYZ max rotational accelerations, xyz rotational velocity.\n",
    "#some axis should be swapped\n",
    "\n",
    "#add extra columns to the dictionary\n",
    "data_dict['peak rot acc resultant'] = []\n",
    "data_dict['peak lin acc resultant'] = []\n",
    "data_dict['peak rot vel resultant'] = []\n",
    "\n",
    "data_dict['peak x rot acc'] = []\n",
    "data_dict['peak y rot acc'] = []\n",
    "data_dict['peak z rot acc'] = []\n",
    "\n",
    "data_dict['peak x rot vel'] = []\n",
    "data_dict['peak y rot vel'] = []\n",
    "data_dict['peak z rot vel'] = []\n",
    "\n",
    "data_dict['peak x lin acc'] = []\n",
    "data_dict['peak y lin acc'] = []\n",
    "data_dict['peak z lin acc'] = []\n",
    "\n",
    "#find maximum values for each impact and desired column in the dictionary\n",
    "for k in range(n):\n",
    "\n",
    "      #access input raw data\n",
    "      path_raw = path_113 + '/' + dir_list[k] + '/' + dir_list[k] + '.csv'\n",
    "      #print(path_raw)\n",
    "      raw_df = pd.read_csv(path_raw)\n",
    "      #reverse required cols\n",
    "      raw_df['la_x'] = - raw_df['la_x']\n",
    "      raw_df['la_z'] = - raw_df['la_z']\n",
    "      raw_df['rv_x'] = - raw_df['rv_x']\n",
    "      raw_df['rv_y'] = - raw_df['rv_y']\n",
    "      raw_df['ra_x'] = - raw_df['ra_x']\n",
    "      raw_df['ra_y'] = - raw_df['ra_y']\n",
    "      \n",
    "      #if k == 0: #printing the first dataframe to make sure it looks correct.\n",
    "      #      print(raw_df)\n",
    "      \n",
    "      #RESULTANTS\n",
    "      #accessing the labelled columns as numpy arrays\n",
    "      raw_ra_r = raw_df['ra_r'].values\n",
    "      raw_la_r = raw_df['la_r'].values \n",
    "      raw_rv_r = raw_df['rv_r'].values\n",
    "      #taking from positive time:\n",
    "      ra_r = raw_ra_r[200:]\n",
    "      la_r = raw_la_r[200:]\n",
    "      rv_r = raw_rv_r[200:]\n",
    "      #finding maximum magnitude values and #appending to the dictionary\n",
    "      data_dict['peak rot acc resultant'].append(ra_r[(np.abs(ra_r)).argmax()])\n",
    "      data_dict['peak lin acc resultant'].append(la_r[(np.abs(la_r)).argmax()])\n",
    "      data_dict['peak rot vel resultant'].append(rv_r[(np.abs(rv_r)).argmax()])\n",
    "\n",
    "      #COMPONENTS:ROT ACC\n",
    "      #accessing the labelled columns as numpy arrays\n",
    "      raw_ra_x = raw_df['ra_x'].values\n",
    "      raw_ra_y = raw_df['ra_y'].values\n",
    "      raw_ra_z = raw_df['ra_z'].values\n",
    "      #taking from positive time\n",
    "      ra_x = raw_ra_x[200:]\n",
    "      ra_y = raw_ra_y[200:]\n",
    "      ra_z = raw_ra_z[200:]\n",
    "      #finding maximum values, appending to dataframe\n",
    "      data_dict['peak x rot acc'].append(ra_x[(np.abs(ra_x)).argmax()])\n",
    "      data_dict['peak y rot acc'].append(ra_y[(np.abs(ra_y)).argmax()])\n",
    "      data_dict['peak z rot acc'].append(ra_z[(np.abs(ra_z)).argmax()])\n",
    "\n",
    "      #COMPONENTS:LIN ACC\n",
    "      #accessing the labelled columns as numpy arrays\n",
    "      raw_la_x = raw_df['la_x'].values\n",
    "      raw_la_y = raw_df['la_y'].values\n",
    "      raw_la_z = raw_df['la_z'].values\n",
    "      #taking from positive time\n",
    "      la_x = raw_la_x[200:]\n",
    "      la_y = raw_la_y[200:]\n",
    "      la_z = raw_la_z[200:]\n",
    "      #finding maximum absolute values, appending to dataframe (argmax returns the index of the largest value)\n",
    "      data_dict['peak x lin acc'].append(la_x[(np.abs(la_x)).argmax()])\n",
    "      data_dict['peak y lin acc'].append(la_y[(np.abs(la_y)).argmax()])\n",
    "      data_dict['peak z lin acc'].append(la_z[(np.abs(la_z)).argmax()])\n",
    "\n",
    "      #COMPONENTS:ROT VEL\n",
    "      #accessing the labelled columns as numpy arrays\n",
    "      raw_rv_x = raw_df['rv_x'].values\n",
    "      raw_rv_y = raw_df['rv_y'].values\n",
    "      raw_rv_z = raw_df['rv_z'].values\n",
    "      #taking from positive time\n",
    "      rv_x = raw_rv_x[200:]\n",
    "      rv_y = raw_rv_y[200:]\n",
    "      rv_z = raw_rv_z[200:]\n",
    "      #finding maximum absolute values, appending to dataframe\n",
    "      data_dict['peak x rot vel'].append(rv_x[(np.abs(rv_r)).argmax()])\n",
    "      data_dict['peak y rot vel'].append(rv_y[(np.abs(rv_y)).argmax()])\n",
    "      data_dict['peak z rot vel'].append(rv_z[(np.abs(rv_z)).argmax()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check with new data\n",
    "impact_no  = random.randint(0, 113)\n",
    "print(data_dict['Impact label'][impact_no])\n",
    "print(data_dict['Strain 90th%'][impact_no])\n",
    "print(data_dict['Strain rate 90th%'][impact_no])\n",
    "print(data_dict['peak rot acc resultant'][impact_no])\n",
    "print(data_dict['peak lin acc resultant'][impact_no])\n",
    "print(data_dict['peak rot vel resultant'][impact_no])\n",
    "print(data_dict['peak x rot acc'][impact_no], '(reversed)') #reversed axis\n",
    "print(data_dict['peak y rot acc'][impact_no], '(reversed)') #reversed axis\n",
    "print(data_dict['peak z rot acc'][impact_no])\n",
    "print(data_dict['peak x rot vel'][impact_no], '(reversed)') #reversed axis\n",
    "print(data_dict['peak y rot vel'][impact_no], '(reversed)') #reversed axis\n",
    "print(data_dict['peak z rot vel'][impact_no])\n",
    "print(data_dict['peak x lin acc'][impact_no], '(reversed)') #reversed axis\n",
    "print(data_dict['peak y lin acc'][impact_no])\n",
    "print(data_dict['peak z lin acc'][impact_no], '(reversed)') #reversed axis\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add IMPACT TYPE 2024 data e.g. nYR, pZR\n",
    "data_dict['Type'] = []\n",
    "\n",
    "for i in range(n): #n defined as number of impacts\n",
    "    #filling in the dictionary by accessing the individual files for each impact\n",
    "    path_raw = path_113 + '/' + dir_list[i] + '/' + dir_list[i] + '.csv' #main path of files + impact path + csv filename\n",
    "    raw_df = pd.read_csv(path_raw,header=None) #data frame extracted from csv (has no column headers)\n",
    "\n",
    "    #individual accessed values assigned to variables\n",
    "    type = raw_df.iat[1,14] #type e.g nYR pZR\n",
    "\n",
    "    #adding values to the dictionary\n",
    "    data_dict['Type'].append(type)\n",
    "\n",
    "    #print('i = ', i,'type =', type) #check outputs look right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOING FOR 2020 DATA.\n",
    "import pandas as pd\n",
    "#FINDING impact strains\n",
    "#STRAIN OUTPUTS/ SIMULATION RESULTS\n",
    "#path_outputs = 'D:/MASTERS/Bicycle_2020/Results/Results_2.csv'\n",
    "path_outputs = 'D:/MASTERS/Bicycle_2020-[original_directory]/Results/Results_3_duplicates_removed.csv'\n",
    "#path_inputs = 'D:/MASTERS/K_files_2020'\n",
    "path_inputs = 'D:/MASTERS/K_files_2020_duplicates_removed'\n",
    "#create empty dictionary:\n",
    "data_dict_2020 = {}\n",
    "#creating the labels for the data\n",
    "#also want impact type (e.g. nYR),and kinematics metrics (e.g max rot acc).\n",
    "data_dict_2020['Impact label'] = []\n",
    "data_dict_2020['Strain 90th%'] = []\n",
    "data_dict_2020['Strain rate 90th%'] = []\n",
    "\n",
    "print(data_dict_2020) #check it looks right\n",
    "\n",
    "#for each line IN THE RESULTS FILE, add label, strain and strain rate. \n",
    "results = pd.read_csv(path_outputs) #opening the results as a dataframe.\n",
    "#print(results)\n",
    "n = len(results)\n",
    "\n",
    "for i in range(n): \n",
    "\n",
    "    #individual accessed values assigned to variables\n",
    "    label = results['name'][i] + '_' + results['test_ID'][i] #label = ABUS GAMECHANJJ_acc_f_01_R_04_X test id is name of input csv.\n",
    "    strain = results['90thPercentileStrainRate'][i] #e.g 0.234 OKAY SO THIS WAS LABELLED WRONG IN THE SPREADSHEET SO STRAIN=STRAINRATE AND VICE VERSA!!!\n",
    "    strainrate = results['90thPercentileStrain'][i] #e.g 0.344\n",
    "\n",
    "    #adding values to the dictionary\n",
    "    data_dict_2020['Impact label'].append(label)\n",
    "    data_dict_2020['Strain 90th%'].append(strain)\n",
    "    data_dict_2020['Strain rate 90th%' ].append(strainrate)\n",
    "\n",
    "    #print('data for', label, 'added') #check dictionary looks right\n",
    "\n",
    "#print(data_dict_2020)\n",
    "print(n)\n",
    "print(data_dict_2020['Impact label'])\n",
    "print(len(data_dict_2020['Impact label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_no  = random.randint(0, 81)\n",
    "print(data_dict_2020['Impact label'][impact_no])\n",
    "print(data_dict_2020['Strain 90th%'][impact_no])\n",
    "print(data_dict_2020['Strain rate 90th%'][impact_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching through all input files, finding peak values\n",
    "# then appending to the dictionary, saving as dataframe.\n",
    "\n",
    "#add extra columns to the dictionary\n",
    "headings = ['peak rot acc resultant','peak lin acc resultant','peak rot vel resultant',\n",
    "            'peak x rot acc','peak y rot acc','peak z rot acc',\n",
    "            'peak x rot vel','peak y rot vel','peak z rot vel',\n",
    "            'peak x lin acc','peak y lin acc','peak z lin acc']\n",
    "for x in headings: #creating empty series in the dictionary.\n",
    "    data_dict_2020[x] = []\n",
    "\n",
    "input_headings = ['ra_r','la_r','rv_r',\n",
    "                  'ra_x','ra_y','ra_z',\n",
    "                  'rv_x','rv_y','rv_z',\n",
    "                  'la_x', 'la_y','la_z']\n",
    "\n",
    "#opening each file in the dataframe:\n",
    "for k in range(n):\n",
    "    impact_file_temp = data_dict_2020['Impact label'][k][-15:] #last 15 characters of the string are the csv that the input kinematics are stored in.\n",
    "    temp_location = path_inputs + '/' + 'FULL_' + impact_file_temp + '.csv'\n",
    "    data = pd.read_csv(temp_location)\n",
    "\n",
    "    #finding peak values for each heading\n",
    "    for j in range(12):\n",
    "        temp_list = data[input_headings[j]].values\n",
    "        index_of_peak_temp = np.abs(temp_list).argmax() #taking index of greatest absolute value\n",
    "        data_dict_2020[headings[j]].append(temp_list[index_of_peak_temp]) #appending to the \n",
    "    #print(impact_file_temp)\n",
    "    #print(temp_location)\n",
    "\n",
    "#adding types, xyz\n",
    "data_dict_2020['Type'] = []\n",
    "for j in range(n):\n",
    "    data_dict_2020['Type'].append(results['test_ID'][j][-1:])\n",
    "\n",
    "#print(pd.DataFrame(data_dict_2020))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a csv with all columns\n",
    "df = pd.DataFrame(data_dict_2020)\n",
    "#df.to_csv('E:/MASTERS/IMPACT_DATA.csv', index=False)\n",
    "#2020 data\n",
    "df.to_csv('D:/MASTERS/IMPACT_DATA_2020_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMBINING THE DATASETS INTO 1 COMBINED DATASET\n",
    "\n",
    "data_df_2020 = pd.read_csv('D:/MASTERS/IMPACT_DATA_2020_2.csv')\n",
    "data_df_2024 = pd.read_csv('D:/MASTERS/IMPACT_DATA.csv')\n",
    "\n",
    "combined_df = pd.concat([data_df_2024, data_df_2020], axis=0, ignore_index=True)\n",
    "\n",
    "combined_df.to_csv('D:/MASTERS/IMPACT_DATA_COMBINED_SWAP.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
